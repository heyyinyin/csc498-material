{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lightweight-deputy"
      },
      "source": [
        "# Assignment 2"
      ],
      "id": "lightweight-deputy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "individual-samba"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from gridworld import StochasticGridworld, DeterministicGridworld, make_random_gridworld\n",
        "from agents import RandomAgent, Agent"
      ],
      "id": "individual-samba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atomic-yesterday"
      },
      "source": [
        "## Task 1"
      ],
      "id": "atomic-yesterday"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "statewide-paris"
      },
      "source": [
        "task = StochasticGridworld()\n",
        "random_agent = RandomAgent(task.observation_space, task.action_space)\n",
        "\n",
        "obs = task.reset()\n",
        "\n",
        "rewards = np.zeros((100, 100))\n",
        "for run in range(100):\n",
        "    for step in range(100):\n",
        "        act = random_agent(obs)\n",
        "        obs, rew, done, info = task.step(act)\n",
        "        rewards[run, step] = rew\n",
        "        \n",
        "print(\"Average return: {}\".format(rewards.sum(1).mean()))\n",
        "print(\"Standard deviation: {}\".format(rewards.sum(1).std()))"
      ],
      "id": "statewide-paris",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "approved-packing"
      },
      "source": [
        "## Task 2A"
      ],
      "id": "approved-packing"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fallen-section"
      },
      "source": [
        "class QLearningAgent(Agent):\n",
        "    \n",
        "    def __init__(self, observation_space, action_space, gamma=0.9):   \n",
        "        self.actions = action_space\n",
        "        self.states = observation_space\n",
        "        \n",
        "        # policy estimate for each state, update in self.q_learning\n",
        "        self.policy = np.zeros((self.states,), dtype=np.int)\n",
        "\n",
        "        # Q-value estimate in each state, update in self.q_learning\n",
        "        self.q_values  = np.zeros((self.states,self.actions), dtype=np.int)\n",
        "        \n",
        "        self.gamma = gamma\n",
        "        \n",
        "    def q_learning(self, states, actions, rewards):\n",
        "        \"\"\"\n",
        "        states: np.array of size (N+1) with state at each time (integer) \n",
        "        The last state is only used to calculate the terminal Q-value; \n",
        "        it is not associated with an update\n",
        "\n",
        "        actions: np.array of size (N) with action at each time (integer)\n",
        "        rewards: np.array of size (N) with single step rewards (float)\n",
        "        \n",
        "        returns nothing, should modify self.policy, self.values in place\n",
        "\n",
        "        \"\"\"\n",
        "        ??? #TODO: Insert code here\n",
        "    \n",
        "    def collect_data(self, task):\n",
        "        # Do not modify\n",
        "        obs = task.reset()\n",
        "\n",
        "        rewards = np.zeros((100,))\n",
        "        states = np.zeros((101,))\n",
        "        actions = np.zeros((100))\n",
        "\n",
        "        for run in range(100):\n",
        "            for step in range(100):\n",
        "                states[step] = obs\n",
        "                act = self.__call__(obs)\n",
        "                obs, rew, done, info = task.step(act)\n",
        "                rewards[step] = rew\n",
        "                actions[step] = act\n",
        "            states[-1] = obs\n",
        "        return states, actions, rewards\n",
        "\n",
        "# Do not modify\n",
        "agent = QLearningAgent(task.observation_space, task.action_space)\n",
        "\n",
        "for i in range(50):\n",
        "  data = agent.collect_data(task)\n",
        "  agent.q_learning(*data)\n",
        "\n",
        "# Final Benchmarking\n",
        "obs = task.reset()\n",
        "\n",
        "rewards = np.zeros((100, 100))\n",
        "states = np.zeros((100, 101))\n",
        "actions = np.zeros((100, 100))\n",
        "\n",
        "for run in range(100):\n",
        "    for step in range(100):\n",
        "        states[run, step] = obs\n",
        "        act = agent(obs)\n",
        "        obs, rew, done, info = task.step(act)\n",
        "        rewards[run, step] = rew\n",
        "        actions[run, step] = act\n",
        "    states[run, -1] = obs\n",
        "        \n",
        "print(\"Average return: {}\".format(rewards.sum(1).mean()))\n",
        "print(\"Standard deviation: {}\".format(rewards.sum(1).std()))"
      ],
      "id": "fallen-section",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aHBxir7U9Ua"
      },
      "source": [
        "## Task 2B"
      ],
      "id": "5aHBxir7U9Ua"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtxKn0m9VBYn"
      },
      "source": [
        "class EpsilonQLearningAgent(QLearningAgent):\r\n",
        "    \r\n",
        "    def __init__(self, observation_space, action_space, gamma=0.9, epsilon=1.0):   \r\n",
        "        self.actions = action_space\r\n",
        "        self.states = observation_space\r\n",
        "        \r\n",
        "        # policy estimate for each state\r\n",
        "        self.policy = np.zeros((self.states,), dtype=np.int)\r\n",
        "        \r\n",
        "        # Q-value estimate in each state\r\n",
        "        self.q_values  = np.zeros((self.states,self.actions), dtype=np.int)\r\n",
        "        \r\n",
        "        self.gamma = gamma\r\n",
        "        self.epsilon = epsilon\r\n",
        "    \r\n",
        "    def epsilon_greedy_policy(self, obs):\r\n",
        "        \"\"\"\r\n",
        "        obs: integer representing state\r\n",
        "\r\n",
        "        returns integer representing action for current state, \r\n",
        "        according to epsilon-greedy policy (see handout)\r\n",
        "\r\n",
        "        epsilon is stored in self.epsilon\r\n",
        "        \r\n",
        "        Hint:\r\n",
        "        act = random_agent(obs) #obtains a random action for obs\r\n",
        "        act = self.__call__(obs) #obtains action according to self.policy\r\n",
        "        \"\"\"\r\n",
        "        ??? #TODO: Insert code here\r\n",
        "\r\n",
        "    def collect_data(self, task):\r\n",
        "        # Do not modify\r\n",
        "        obs = task.reset()\r\n",
        "\r\n",
        "        rewards = np.zeros((100,))\r\n",
        "        states = np.zeros((101,))\r\n",
        "        actions = np.zeros((100))\r\n",
        "\r\n",
        "        for run in range(100):\r\n",
        "            for step in range(100):\r\n",
        "                states[step] = obs\r\n",
        "                act = self.epsilon_greedy_policy(obs)\r\n",
        "                obs, rew, done, info = task.step(act)\r\n",
        "                rewards[step] = rew\r\n",
        "                actions[step] = act\r\n",
        "            states[-1] = obs\r\n",
        "        return states, actions, rewards"
      ],
      "id": "LtxKn0m9VBYn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF7nOdGnwS-w"
      },
      "source": [
        "# Substitute different values for epsilon below, comment on final performance\r\n",
        "EPSILON = 1.0\r\n",
        "\r\n",
        "# Do not modify\r\n",
        "agent = EpsilonQLearningAgent(task.observation_space, task.action_space, epsilon=EPSILON)\r\n",
        "\r\n",
        "for i in range(50):\r\n",
        "  data = agent.collect_data(task)\r\n",
        "  agent.q_learning(*data)\r\n",
        "\r\n",
        "# Final Benchmarking\r\n",
        "obs = task.reset()\r\n",
        "\r\n",
        "rewards = np.zeros((100, 100))\r\n",
        "states = np.zeros((100, 101))\r\n",
        "actions = np.zeros((100, 100))\r\n",
        "\r\n",
        "for run in range(100):\r\n",
        "    for step in range(100):\r\n",
        "        states[run, step] = obs\r\n",
        "        act = agent(obs)\r\n",
        "        obs, rew, done, info = task.step(act)\r\n",
        "        rewards[run, step] = rew\r\n",
        "        actions[run, step] = act\r\n",
        "    states[run, -1] = obs\r\n",
        "        \r\n",
        "print(\"Epsilon: {}\".format(EPSILON))\r\n",
        "print(\"Average return: {}\".format(rewards.sum(1).mean()))\r\n",
        "print(\"Standard deviation: {}\".format(rewards.sum(1).std()))"
      ],
      "id": "TF7nOdGnwS-w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5CW23n1u7QX"
      },
      "source": [
        "How does the model perform for different values of epsilon?"
      ],
      "id": "d5CW23n1u7QX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bacterial-heater"
      },
      "source": [
        "## Task 3A\n",
        "\n",
        "You can reuse code here where applicable"
      ],
      "id": "bacterial-heater"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "perfect-taxation"
      },
      "source": [
        "class TDAgent(Agent):\n",
        "    \n",
        "    def __init__(self, observation_space, action_space, gamma=0.9):   \n",
        "        self.actions = action_space\n",
        "        self.states = observation_space\n",
        "        \n",
        "        self.policy = np.zeros((self.states,), dtype=np.int)\n",
        "        self.values  = np.zeros((self.states,), dtype=np.int)\n",
        "        \n",
        "        self.gamma = gamma\n",
        "    def td_learning(self, states, actions, rewards):\n",
        "        \"\"\"\n",
        "        states: np.array of size (N+1) with state at each time (integer) \n",
        "        The last state is only used to calculate the terminal Q-value; \n",
        "        it is not associated with an update\n",
        "\n",
        "        actions: np.array of size (N) with action at each time (integer)\n",
        "        rewards: np.array of size (N) with single step rewards (float)\n",
        "        \n",
        "        returns nothing, should modify self.policy, self.values in place\n",
        "        uses the TD(0) algorithm described in the lectures\n",
        "        \"\"\"\n",
        "        ??? #TODO: Insert code here\n",
        "\n",
        "    def collect_data(self, task):\n",
        "        # Do not modify\n",
        "        obs = task.reset()\n",
        "\n",
        "        rewards = np.zeros((100,))\n",
        "        states = np.zeros((101,))\n",
        "        actions = np.zeros((100))\n",
        "\n",
        "        for run in range(100):\n",
        "            for step in range(100):\n",
        "                states[step] = obs\n",
        "                act = self(obs)\n",
        "                obs, rew, done, info = task.step(act)\n",
        "                rewards[step] = rew\n",
        "                actions[step] = act\n",
        "            states[-1] = obs\n",
        "        return states, actions, rewards\n",
        "        \n",
        "# Do not modify\n",
        "agent = TDAgent(task.observation_space, task.action_space)\n",
        "\n",
        "for i in range(50):\n",
        "  data = agent.collect_data(task)\n",
        "  agent.td_learning(*data)\n",
        "\n",
        "# Final Benchmarking\n",
        "obs = task.reset()\n",
        "\n",
        "rewards = np.zeros((100, 100))\n",
        "states = np.zeros((100, 101))\n",
        "actions = np.zeros((100, 100))\n",
        "\n",
        "for run in range(100):\n",
        "    for step in range(100):\n",
        "        states[run, step] = obs\n",
        "        act = agent(obs)\n",
        "        obs, rew, done, info = task.step(act)\n",
        "        rewards[run, step] = rew\n",
        "        actions[run, step] = act\n",
        "    states[run, -1] = obs\n",
        "        \n",
        "print(\"Average return: {}\".format(rewards.sum(1).mean()))\n",
        "print(\"Standard deviation: {}\".format(rewards.sum(1).std()))"
      ],
      "id": "perfect-taxation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vocal-tracker"
      },
      "source": [
        "Do the results conform with your expectations? Write a small discussion."
      ],
      "id": "vocal-tracker"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "varied-individual"
      },
      "source": [
        "class TD_nstep_Agent(TDAgent):\r\n",
        "    \r\n",
        "    def __init__(self, observation_space, action_space, gamma=0.9, nstep=0):   \r\n",
        "        self.actions = action_space\r\n",
        "        self.states = observation_space\r\n",
        "        \r\n",
        "        self.policy = np.zeros((self.states,), dtype=np.int)\r\n",
        "        self.values  = np.zeros((self.states,), dtype=np.int)\r\n",
        "        \r\n",
        "        self.gamma = gamma\r\n",
        "\r\n",
        "        #\r\n",
        "        self.nstep = nstep \r\n",
        "\r\n",
        "    def td_learning(self, states, actions, rewards):\r\n",
        "        \"\"\"\r\n",
        "        states: np.array of size (N+1) with state at each time (integer) \r\n",
        "        The last state is only used to calculate the terminal Q-value; \r\n",
        "        it is not associated with an update\r\n",
        "\r\n",
        "        actions: np.array of size (N) with action at each time (integer)\r\n",
        "        rewards: np.array of size (N) with single step rewards (float)\r\n",
        "        \r\n",
        "        returns nothing, should modify self.policy, self.values in place\r\n",
        "        uses the TD nstep algorithm described in the handout\r\n",
        "\r\n",
        "        The nstep can be found in self.nstep, self.nstep = 0 should correspond\r\n",
        "        to TDAgent\r\n",
        "\r\n",
        "        Hint: if there are only k < nstep future observations available for\r\n",
        "        a given time, then simply use k instead of nstep for that time.\r\n",
        "\r\n",
        "        \"\"\"\r\n",
        "        ??? #TODO: Insert code here"
      ],
      "id": "varied-individual",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6mTnjMNyqAa"
      },
      "source": [
        "# Set nstep here:\r\n",
        "NSTEP = 0\r\n",
        "\r\n",
        "# Do not modify\r\n",
        "agent = TD_nstep_Agent(task.observation_space, task.action_space, nstep=NSTEP)\r\n",
        "\r\n",
        "for i in range(50):\r\n",
        "  data = agent.collect_data(task)\r\n",
        "  agent.td_learning(*data)\r\n",
        "\r\n",
        "# Final Benchmarking\r\n",
        "obs = task.reset()\r\n",
        "\r\n",
        "rewards = np.zeros((100, 100))\r\n",
        "states = np.zeros((100, 101))\r\n",
        "actions = np.zeros((100, 100))\r\n",
        "\r\n",
        "for run in range(100):\r\n",
        "    for step in range(100):\r\n",
        "        states[run, step] = obs\r\n",
        "        act = agent(obs)\r\n",
        "        obs, rew, done, info = task.step(act)\r\n",
        "        rewards[run, step] = rew\r\n",
        "        actions[run, step] = act\r\n",
        "    states[run, -1] = obs\r\n",
        "\r\n",
        "print(\"Nsteps: {}\".format(NSTEP))    \r\n",
        "print(\"Average return: {}\".format(rewards.sum(1).mean()))\r\n",
        "print(\"Standard deviation: {}\".format(rewards.sum(1).std()))"
      ],
      "id": "-6mTnjMNyqAa",
      "execution_count": null,
      "outputs": []
    }
  ]
}